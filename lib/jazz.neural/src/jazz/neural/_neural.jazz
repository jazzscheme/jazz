;;;==============
;;;  JazzScheme
;;;==============
;;;
;;;; Neural Networks
;;;
;;;  The contents of this file are subject to the Mozilla Public License Version
;;;  1.1 (the "License"); you may not use this file except in compliance with
;;;  the License. You may obtain a copy of the License at
;;;  http://www.mozilla.org/MPL/
;;;
;;;  Software distributed under the License is distributed on an "AS IS" basis,
;;;  WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
;;;  for the specific language governing rights and limitations under the
;;;  License.
;;;
;;;  The Original Code is JazzScheme.
;;;
;;;  The Initial Developer of the Original Code is Guillaume Cartier.
;;;  Portions created by the Initial Developer are Copyright (C) 1996-2018
;;;  the Initial Developer. All Rights Reserved.
;;;
;;;  Contributor(s):
;;;
;;;  Alternatively, the contents of this file may be used under the terms of
;;;  the GNU General Public License Version 2 or later (the "GPL"), in which
;;;  case the provisions of the GPL are applicable instead of those above. If
;;;  you wish to allow use of your version of this file only under the terms of
;;;  the GPL, and not to allow others to use your version of this file under the
;;;  terms of the MPL, indicate your decision by deleting the provisions above
;;;  and replace them with the notice and other provisions required by the GPL.
;;;  If you do not delete the provisions above, a recipient may use your version
;;;  of this file under the terms of any one of the MPL or the GPL.
;;;
;;;  See www.jazzscheme.org for details.


(module jazz.neural jazz:


(export (jazz.neural.activation)
        (jazz.neural.data)
        (jazz.neural.io)
        (jazz.neural.layer)
        (jazz.neural.loss)
        (jazz.neural.math)
        (jazz.neural.optimizer)
        (jazz.neural.overload)
        (jazz.neural.tensor))

(import (jazz (except + * - / expt exp log map max sqrt))
        (jazz.neural.activation)
        (jazz.neural.data)
        (jazz.neural.io)
        (jazz.neural.layer)
        (jazz.neural.loss)
        (jazz.neural.math)
        (jazz.neural.optimizer)
        (jazz.neural.overload)
        (jazz.neural.tensor)
        (jazz.settings))


(definition public neural-debug?
  (boolean-argument "debug" #f))

(definition public (set-neural-debug? debug?)
  (set! neural-debug? debug?))


(class Model extends Object
  
  
  (slot input         getter generate)
  (slot layers        getter generate)
  (slot optimizer     getter generate)
  (slot loss-function getter generate)
  
  
  (method override (initialize self input-dimension optimizer loss-function)
    (nextmethod self)
    (set! self.input (new Layer input-dimension))
    (set! self.layers '())
    (set! self.optimizer optimizer)
    (set! self.loss-function loss-function))
  
  
  (method override (print self output readably)
    (print-unreadable self output
      (lambda (output)
        (let ((layers (length layers)))
          (format output "{a} layer{a}" layers (format-plural layers))))))
  
  
  (method public (add self layer)
    (setup layer (if (null? layers) input (last layers)) optimizer)
    (set! layers (append! layers (list layer))))
  
  
  (method public (feed-forward self X training?)
    (let ((name 'Input))
    (let ((layer-output X))
      (for-each (lambda (layer)
                  (when (and neural-debug? training?)
                    (tell '-> name)
                    (tell layer-output))
                  (set! name (present layer))
                  (set! layer-output (forward-pass layer layer-output training?)))
                layers)
      (when (and neural-debug? training?)
        (tell '-> name)
        (tell layer-output))
      layer-output)))
  
  
  (method public (propagate-backward self loss-grad)
    (for-each-reversed (lambda (layer)
                         (set! loss-grad (backward-pass layer loss-grad))
                         (when neural-debug?
                           (tell '<- 'loss-grad (present layer))
                           (tell loss-grad)))
                       layers))
  
  
  (method public (train self X y)
    (let ((y-pred (feed-forward self X #t)))
      (let ((loss-grad (loss-gradient loss-function y y-pred)))
        (when neural-debug?
          (tell '<- 'loss-grad 'Output)
          (tell loss-grad))
        (propagate-backward self loss-grad))))
  
  
  (method public (fit self X y (epochs: epochs 1) (batch-size: batch-size 64) (feedback: feedback 100))
    (let ((batches (split-batches X y batch-size)))
      (loop (for epoch from 1 to epochs)
            (do (unless neural-debug?
                  (when (and feedback (= (modulo epoch feedback) 0))
                    (tell epoch)))
                (for-each (lambda (batch)
                            (bind (X . y) batch
                              (train self X y)))
                          batches)))))
  
  
  (method public (test self X y)
    (let ((y-pred (feed-forward self X #f)))
      (let ((loss (mean (loss loss-function y y-pred)))
            (acc (accuracy loss-function y y-pred)))
        (values loss acc))))
  
  
  (method public (predict self X)
    (feed-forward self X #f))))
