;;;==============
;;;  JazzScheme
;;;==============
;;;
;;;; Activation Functions
;;;
;;;  The contents of this file are subject to the Mozilla Public License Version
;;;  1.1 (the "License"); you may not use this file except in compliance with
;;;  the License. You may obtain a copy of the License at
;;;  http://www.mozilla.org/MPL/
;;;
;;;  Software distributed under the License is distributed on an "AS IS" basis,
;;;  WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
;;;  for the specific language governing rights and limitations under the
;;;  License.
;;;
;;;  The Original Code is JazzScheme.
;;;
;;;  The Initial Developer of the Original Code is Guillaume Cartier.
;;;  Portions created by the Initial Developer are Copyright (C) 1996-2018
;;;  the Initial Developer. All Rights Reserved.
;;;
;;;  Contributor(s):
;;;
;;;  Alternatively, the contents of this file may be used under the terms of
;;;  the GNU General Public License Version 2 or later (the "GPL"), in which
;;;  case the provisions of the GPL are applicable instead of those above. If
;;;  you wish to allow use of your version of this file only under the terms of
;;;  the GPL, and not to allow others to use your version of this file under the
;;;  terms of the MPL, indicate your decision by deleting the provisions above
;;;  and replace them with the notice and other provisions required by the GPL.
;;;  If you do not delete the provisions above, a recipient may use your version
;;;  of this file under the terms of any one of the MPL or the GPL.
;;;
;;;  See www.jazzscheme.org for details.


(module protected jazz.neural.activation jazz:


(import (jazz (except + * - / expt exp log map max sqrt))
        (jazz.neural.io)
        (jazz.neural.overload)
        (jazz.neural.tensor))


;;;
;;;; Activation-Function
;;;


(class Activation-Function extends Object
  
  
  (method public virtual (activate self x)
    )
  
  
  (method public virtual (activate-gradient self x)
    ))


;;;
;;;; Sigmoid
;;;


(class Sigmoid extends Activation-Function
  
  
  (method override (activate self x)
    (/ 1 (+ 1 (exp (- x)))))
  
  
  (method override (activate-gradient self x)
    (let ((val (activate self x)))
      (* val (- 1 val)))))


(definition public sigmoid
  (new Sigmoid))


;;;
;;;; Softmax
;;;


(class Softmax extends Activation-Function
  
  
  (method override (activate self x)
    (let ((e-x (exp (- x (max x axis: 1)))))
      (/ e-x (sum e-x axis: 1))))

  
  (method override (activate-gradient self x)
    (let ((val (activate self x)))
      (* val (- 1 val)))))


(definition public softmax
  (new Softmax))


;;;
;;;; ReLU
;;;


(class ReLU extends Activation-Function
  
  
  (method override (activate self x)
    (map (lambda (x) (if (>= x 0) x 0)) x))

  
  (method override (activate-gradient self x)
    (map (lambda (x) (if (>= x 0) 1 0)))))


(definition public relu
  (new ReLU))


;;;
;;;; Leaky-ReLU
;;;


(class Leaky-ReLU extends Activation-Function
  
  
  (slot alpha getter generate)
  
  
  (method override (initialize self (alpha: alpha 0.2))
    (set! self.alpha alpha))

  
  (method override (activate self x)
    (map (lambda (x) (if (>= x 0) x (* alpha x))) x))
  

  (method override (activate-gradient self x)
    (map (lambda (x) (if (>= x 0) 1 alpha)) x)))


(definition public leaky-relu
  (new Leaky-ReLU)))
