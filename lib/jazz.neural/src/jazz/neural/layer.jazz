;;;==============
;;;  JazzScheme
;;;==============
;;;
;;;; Neural Network Layers
;;;
;;;  The contents of this file are subject to the Mozilla Public License Version
;;;  1.1 (the "License"); you may not use this file except in compliance with
;;;  the License. You may obtain a copy of the License at
;;;  http://www.mozilla.org/MPL/
;;;
;;;  Software distributed under the License is distributed on an "AS IS" basis,
;;;  WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
;;;  for the specific language governing rights and limitations under the
;;;  License.
;;;
;;;  The Original Code is JazzScheme.
;;;
;;;  The Initial Developer of the Original Code is Guillaume Cartier.
;;;  Portions created by the Initial Developer are Copyright (C) 1996-2018
;;;  the Initial Developer. All Rights Reserved.
;;;
;;;  Contributor(s):
;;;
;;;  Alternatively, the contents of this file may be used under the terms of
;;;  the GNU General Public License Version 2 or later (the "GPL"), in which
;;;  case the provisions of the GPL are applicable instead of those above. If
;;;  you wish to allow use of your version of this file only under the terms of
;;;  the GPL, and not to allow others to use your version of this file under the
;;;  terms of the MPL, indicate your decision by deleting the provisions above
;;;  and replace them with the notice and other provisions required by the GPL.
;;;  If you do not delete the provisions above, a recipient may use your version
;;;  of this file under the terms of any one of the MPL or the GPL.
;;;
;;;  See www.jazzscheme.org for details.


(module protected jazz.neural.layer jazz


(import (jazz.neural.activation)
        (jazz.neural.io)
        (jazz.neural.math)
        (jazz.neural.optimizer)
        (jazz.neural.settings)
        (jazz.neural.syntax (phase syntax))
        (jazz.neural.tensor))


(proclaim (not check bounds types)
          (warn optimizations))


;;;
;;;; Layer
;;;


(class Layer extends Object
  
  
  (slot input-dimension getter generate)
  (slot dimension       getter generate)
  
  
  (method override (initialize self dimension)
    (nextmethod self)
    (set! self.dimension dimension))
  
  
  (method protected virtual (setup self input optimizer)
    (set! input-dimension (get-dimension input)))
  
  
  (method override (print self output readably)
    (print-unreadable self output
      (lambda (output)
        (format output "{a}" dimension))))
  
  
  (method protected virtual (present self)
    (symbol->string (category-name (class-of self))))
  
  
  (method public virtual (forward-pass self X training)
    )

  
  (method public virtual (backward-pass self accum-grad)
    )
  
  
  (method public virtual (unref-layer self)
    ))


;;;
;;;; Dense
;;;


(class Dense extends Layer
  
  
  (slot layer-input getter generate)
  (slot W           getter generate)
  (slot w0          getter generate)
  (slot W-opt       getter generate)
  (slot w0-opt      getter generate)
  
  
  (method override (setup self input optimizer)
    (nextmethod self input optimizer)
    (let ((limit (/ 1. (sqrt input-dimension))))
      (set& self.W (random-tensor-between input-dimension dimension (- limit) limit))
      (when neural-print?
        (tell '<< (present self))
        (tell self.W))
      (set& self.w0 (zero-tensor 1 dimension))
      (set! self.W-opt (clone optimizer))
      (set! self.w0-opt (clone optimizer))))
  
  
  (method override (forward-pass self X <Tensor> training?) <Tensor>
    (with& (X)
      (set& layer-input X)
      (tensor+ (tensor-dot X W) w0)))
  
  
  (method override (backward-pass self accum-grad <Tensor>) <Tensor>
    (with& (accum-grad)
      ;; save weights used during forwards pass
      (let& ((W W))
        ;; calculate gradient w.r.t layer weights
        (let& ((grad-w (tensor-dot (tensor-transpose layer-input) accum-grad))
               (grad-w0 (tensor-sum accum-grad axis: 0)))
          ;; update the layer weights
          (set& self.W (update W-opt self.W grad-w))
          (set& self.w0 (update w0-opt self.w0 grad-w0))
          ;; return accumulated gradient for next layer calculated
          ;; based on the weights used during the forward pass
          (tensor-dot accum-grad (tensor-transpose W))))))
  
  
  (method override (unref-layer self)
    (unref layer-input)
    (unref W)
    (unref w0)
    (unref-optimizer W-opt)
    (unref-optimizer w0-opt)))


;;;
;;;; Dropout
;;;


(class Dropout extends Layer
  
  
  (slot p    <fl> getter generate)
  (slot mask      getter generate)
  
  
  (method override (initialize self p <fl>)
    (set! self.p p)
    (set& self.mask #f))
  
  
  (method override (setup self input optimizer)
    (nextmethod self input optimizer)
    (set! self.dimension input-dimension))

  
  (method override (forward-pass self X <Tensor> training?) <Tensor>
    (with& (X)
      (cond (training?
             (set& mask (tensor-dropout p (random-tensor (tensor-rows X) (tensor-cols X))))
             (tensor* X mask))
            (else
             (tensor-scalar* (- 1. p) X)))))

  
  (method override (backward-pass self accum-grad <Tensor>) <Tensor>
    (with& (accum-grad)
      (tensor* accum-grad mask)))
  
  
  (method override (unref-layer self)
    (when mask
      (unref mask))))


;;;
;;;; Activation
;;;


(class Activation extends Layer
  
  
  (slot layer-input getter generate)
  (slot activation  getter generate)
  
  
  (method override (initialize self activation)
    (set! self.activation activation))
  
  
  (method override (setup self input optimizer)
    (nextmethod self input optimizer)
    (set! self.dimension input-dimension))
  
  
  (method override (print self output readably)
    (print-unreadable self output
      (lambda (output)
        (format output "{a}" (category-name (class-of activation))))))
  
  
  (method override (present self)
    (format "{a} ({a})" (nextmethod self) (category-name (class-of activation))))

  
  (method override (forward-pass self X <Tensor> training?) <Tensor>
    (with& (X)
      (set& layer-input X)
      (activate activation X)))

  
  (method override (backward-pass self accum-grad <Tensor>) <Tensor>
    (with& (accum-grad)
      (tensor* accum-grad (activate-gradient activation layer-input))))
  
  
  (method override (unref-layer self)
    (unref layer-input))))
