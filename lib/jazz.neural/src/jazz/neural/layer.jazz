;;;==============
;;;  JazzScheme
;;;==============
;;;
;;;; Neural Network Layers
;;;
;;;  The contents of this file are subject to the Mozilla Public License Version
;;;  1.1 (the "License"); you may not use this file except in compliance with
;;;  the License. You may obtain a copy of the License at
;;;  http://www.mozilla.org/MPL/
;;;
;;;  Software distributed under the License is distributed on an "AS IS" basis,
;;;  WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
;;;  for the specific language governing rights and limitations under the
;;;  License.
;;;
;;;  The Original Code is JazzScheme.
;;;
;;;  The Initial Developer of the Original Code is Guillaume Cartier.
;;;  Portions created by the Initial Developer are Copyright (C) 1996-2018
;;;  the Initial Developer. All Rights Reserved.
;;;
;;;  Contributor(s):
;;;
;;;  Alternatively, the contents of this file may be used under the terms of
;;;  the GNU General Public License Version 2 or later (the "GPL"), in which
;;;  case the provisions of the GPL are applicable instead of those above. If
;;;  you wish to allow use of your version of this file only under the terms of
;;;  the GPL, and not to allow others to use your version of this file under the
;;;  terms of the MPL, indicate your decision by deleting the provisions above
;;;  and replace them with the notice and other provisions required by the GPL.
;;;  If you do not delete the provisions above, a recipient may use your version
;;;  of this file under the terms of any one of the MPL or the GPL.
;;;
;;;  See www.jazzscheme.org for details.


(module protected jazz.neural.layer jazz:


(import (jazz (except + * - / expt exp log map max sqrt))
        (jazz.neural.activation)
        (jazz.neural.io)
        (jazz.neural.math)
        (jazz.neural.optimizer)
        (jazz.neural.overload)
        (jazz.neural.tensor))


;;;
;;;; Layer
;;;


(class Layer extends Object
  
  
  (slot input-dimension getter generate)
  (slot dimension       getter generate)
  
  
  (method override (initialize self dimension)
    (nextmethod self)
    (set! self.dimension dimension))
  
  
  (method protected virtual (setup self input optimizer)
    (set! input-dimension (get-dimension input)))
  
  
  (method override (print self output readably)
    (print-unreadable self output
      (lambda (output)
        (format output "{a}" dimension))))
  
  
  (method protected virtual (present self)
    (symbol->string (category-name (class-of self))))
  
  
  (method public virtual (forward-pass self X training)
    )

  
  (method public virtual (backward-pass self accum-grad)
    ))


;;;
;;;; Dense
;;;


(class Dense extends Layer
  
  
  (slot layer-input getter generate)
  (slot W           getter generate)
  (slot w0          getter generate)
  (slot W-opt       getter generate)
  (slot w0-opt      getter generate)
  
  
  (method override (setup self input optimizer)
    (nextmethod self input optimizer)
    (let ((limit (/ 1. (sqrt input-dimension))))
      (set! self.W (random-tensor input-dimension dimension (- limit) limit))
      (set! self.w0 (zero-tensor 1 dimension))
      (set! self.W-opt (clone optimizer))
      (set! self.w0-opt (clone optimizer))))
  
  
  (method override (forward-pass self X training?)
    (set! layer-input X)
    (+ (dot X W) w0))
  
  
  (method override (backward-pass self accum-grad)
    ;; save weights used during forwards pass
    (let ((W W))
      ;; calculate gradient w.r.t layer weights
      (let ((grad-w (dot (transpose layer-input) accum-grad))
            (grad-w0 (sum accum-grad axis: 0)))
        ;; update the layer weights
        (set! self.W (update W-opt self.W grad-w))
        (set! self.w0 (update w0-opt self.w0 grad-w0))
        ;; return accumulated gradient for next layer calculated
        ;; based on the weights used during the forward pass
        (dot accum-grad (transpose W))))))


;;;
;;;; Dropout
;;;


(class Dropout extends Layer
  
  
  (slot p    <fl> getter generate)
  (slot mask      getter generate)
  
  
  (method override (initialize self p)
    (set! self.p p)
    (set! self.mask #f))
  
  
  (method override (setup self input optimizer)
    (nextmethod self input optimizer)
    (set! self.dimension input-dimension))

  
  (method override (forward-pass self X training?)
    (let ((c (if (not training?)
                 (- 1. p)
               (set! mask (map (lambda (x <fl>) (if (> x p) 1. 0.)) (random-tensor (tensor-rows X) (tensor-cols X))))
               mask)))
      (* X c)))

  
  (method override (backward-pass self accum-grad)
    (* accum-grad mask)))


;;;
;;;; Activation
;;;


(class Activation extends Layer
  
  
  (slot layer-input getter generate)
  (slot activation  getter generate)
  
  
  (method override (initialize self activation)
    (set! self.activation activation))
  
  
  (method override (setup self input optimizer)
    (nextmethod self input optimizer)
    (set! self.dimension input-dimension))
  
  
  (method override (print self output readably)
    (print-unreadable self output
      (lambda (output)
        (format output "{a}" (category-name (class-of activation))))))
  
  
  (method override (present self)
    (format "{a} ({a})" (nextmethod self) (category-name (class-of activation))))

  
  (method override (forward-pass self X training?)
    (set! layer-input X)
    (activate activation X))

  
  (method override (backward-pass self accum-grad)
    (* accum-grad (activate-gradient activation layer-input)))))
