;;;==============
;;;  JazzScheme
;;;==============
;;;
;;;; Optimizers
;;;
;;;  The contents of this file are subject to the Mozilla Public License Version
;;;  1.1 (the "License"); you may not use this file except in compliance with
;;;  the License. You may obtain a copy of the License at
;;;  http://www.mozilla.org/MPL/
;;;
;;;  Software distributed under the License is distributed on an "AS IS" basis,
;;;  WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
;;;  for the specific language governing rights and limitations under the
;;;  License.
;;;
;;;  The Original Code is JazzScheme.
;;;
;;;  The Initial Developer of the Original Code is Guillaume Cartier.
;;;  Portions created by the Initial Developer are Copyright (C) 1996-2018
;;;  the Initial Developer. All Rights Reserved.
;;;
;;;  Contributor(s):
;;;
;;;  Alternatively, the contents of this file may be used under the terms of
;;;  the GNU General Public License Version 2 or later (the "GPL"), in which
;;;  case the provisions of the GPL are applicable instead of those above. If
;;;  you wish to allow use of your version of this file only under the terms of
;;;  the GPL, and not to allow others to use your version of this file under the
;;;  terms of the MPL, indicate your decision by deleting the provisions above
;;;  and replace them with the notice and other provisions required by the GPL.
;;;  If you do not delete the provisions above, a recipient may use your version
;;;  of this file under the terms of any one of the MPL or the GPL.
;;;
;;;  See www.jazzscheme.org for details.


(module protected jazz.neural.optimizer jazz:


(import (jazz (except + * - / expt exp log map max sqrt))
        (jazz.neural.array)
        (jazz.neural.overload))


;;;
;;;; Optimizer
;;;


(class Optimizer extends Object
  
  
  (method public virtual (clone self)
    )
  
  
  (method public virtual (update self w grad-wrt-w)
    ))


;;;
;;;; Stochastic-Gradient-Descent
;;;


(class Stochastic-Gradient-Descent extends Optimizer
  
  
  (slot learning-rate getter generate)
  (slot momentum      getter generate)
  (slot w-updt        getter generate)
  
  
  (method override (initialize self (learning-rate: learning-rate 0.01) (momentum: momentum #f))
    (nextmethod self)
    (set! self.learning-rate learning-rate)
    (set! self.momentum momentum)
    (set! self.w-updt #f))
  
  
  (method override (clone self)
    (let ((copy (new (class-of self))))
      (set-slot-value copy 'learning-rate learning-rate)
      (set-slot-value copy 'w-updt w-updt)
      copy))
  
  
  (method override (update self w grad-wrt-w)
    ;; if not initialized
    (when (and momentum (not w-updt))
      (set! w-updt (zero-array (array-rows w) (array-cols w))))
    ;; use momentum if set
    (if momentum
        (set! w-updt (+ (* momentum w-updt) (* (- 1. momentum) grad-wrt-w)))
      (set! w-updt grad-wrt-w))
    ;; move against the gradient to minimize loss
    (- w (* learning-rate w-updt))))


(definition public sgd
  (new Stochastic-Gradient-Descent))


;;;
;;;; Adam
;;;


(class Adam extends Optimizer
  
  
  (slot learning-rate getter generate)
  (slot eps           getter generate)
  (slot m             getter generate)
  (slot v             getter generate)
  (slot b1            getter generate)
  (slot b2            getter generate)
  (slot w-updt        getter generate)
  
  
  (method override (initialize self (learning-rate: learning-rate 0.001) (b1: b1 0.9) (b2: b2 0.999))
    (nextmethod self)
    (set! self.learning-rate learning-rate)
    (set! self.eps 1e-8)
    (set! self.m #f)
    (set! self.v #f)
    ;; decay rates
    (set! self.b1 b1)
    (set! self.b2 b2)
    (set! self.w-updt #f))
  
  
  (method override (clone self)
    (let ((copy (new (class-of self))))
      (set-slot-value copy 'learning-rate learning-rate)
      (set-slot-value copy 'eps eps)
      (set-slot-value copy 'm m)
      (set-slot-value copy 'v v)
      (set-slot-value copy 'b1 b1)
      (set-slot-value copy 'b2 b2)
      (set-slot-value copy 'w-updt w-updt)
      copy))
  

  (method override (update self w grad-wrt-w)
    ;; if not initialized
    (when (not m)
      (set! m (zero (shape grad-wrt-w)))
      (set! v (zero (shape grad-wrt-w))))
    (set! m (+ (* b1 m) (* (- 1 b1) grad-wrt-w)))
    (set! v (+ (* b2 v) (* (- 1 b2) (expt grad-wrt-w 2))))
    (let ((m-hat (/ m (- 1 b1)))
          (v-hat (/ v (- 1 b2))))
      (set! w-updt (/ (* learning-rate m-hat) (+ (sqrt v-hat) eps))))
    (- w w-updt)))


(definition public adam
  (new Adam)))
 