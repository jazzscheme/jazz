;;;==============
;;;  JazzScheme
;;;==============
;;;
;;;; Optimizers
;;;
;;;  The contents of this file are subject to the Mozilla Public License Version
;;;  1.1 (the "License"); you may not use this file except in compliance with
;;;  the License. You may obtain a copy of the License at
;;;  http://www.mozilla.org/MPL/
;;;
;;;  Software distributed under the License is distributed on an "AS IS" basis,
;;;  WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
;;;  for the specific language governing rights and limitations under the
;;;  License.
;;;
;;;  The Original Code is JazzScheme.
;;;
;;;  The Initial Developer of the Original Code is Guillaume Cartier.
;;;  Portions created by the Initial Developer are Copyright (C) 1996-2018
;;;  the Initial Developer. All Rights Reserved.
;;;
;;;  Contributor(s):
;;;
;;;  Alternatively, the contents of this file may be used under the terms of
;;;  the GNU General Public License Version 2 or later (the "GPL"), in which
;;;  case the provisions of the GPL are applicable instead of those above. If
;;;  you wish to allow use of your version of this file only under the terms of
;;;  the GPL, and not to allow others to use your version of this file under the
;;;  terms of the MPL, indicate your decision by deleting the provisions above
;;;  and replace them with the notice and other provisions required by the GPL.
;;;  If you do not delete the provisions above, a recipient may use your version
;;;  of this file under the terms of any one of the MPL or the GPL.
;;;
;;;  See www.jazzscheme.org for details.


(module protected jazz.neural.optimizer jazz


(import (jazz.neural.array))


;;;
;;;; Optimizer
;;;


(class Optimizer extends Object
  
  
  (method public virtual (clone self)
    )
  
  
  (method public virtual (update self w grad-wrt-w)
    ))


;;;
;;;; Stochastic-Gradient-Descent
;;;


(class Stochastic-Gradient-Descent extends Optimizer
  
  
  (slot learning-rate getter generate)
  (slot w-updt        getter generate)
  
  
  (method override (initialize self (learning-rate: learning-rate 0.01))
    (nextmethod self)
    (set! self.learning-rate learning-rate)
    (set! self.w-updt #f))
  
  
  (method override (clone self)
    (let ((copy (new (class-of self))))
      (set-slot-value copy 'learning-rate learning-rate)
      (set-slot-value copy 'w-updt w-updt)
      copy))
  
  
  (method override (update self w grad-wrt-w)
    @not-needed-at-the-moment-comes-from-having-momentum
    (when (not w-updt)
      (set! w-updt (zero-array (array-rows w) (array-cols w))))
    ;; no momentum for the moment
    (set! w-updt grad-wrt-w)
    ;; move against the gradient to minimize loss
    (array- w (array-scalar* learning-rate w-updt))))


(definition public sgd
  (new Stochastic-Gradient-Descent)))
 